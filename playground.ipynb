{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf41d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/ujjwal.upadhyay/miniconda3/envs/trans/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1ab411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa74c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_NUMBER_OF_LAYERS = 6\n",
    "BASELINE_MODEL_DIMENSION = 512\n",
    "BASELINE_MODEL_NUMBER_OF_HEADS = 8\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
    "\n",
    "\n",
    "BIG_MODEL_NUMBER_OF_LAYERS = 6\n",
    "BIG_MODEL_DIMENSION = 1024\n",
    "BIG_MODEL_NUMBER_OF_HEADS = 16\n",
    "BIG_MODEL_DROPOUT_PROB = 0.3\n",
    "BIG_MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
    "\n",
    "\n",
    "BOS_TOKEN = '[CLS]'\n",
    "EOS_TOKEN = '[SEP]'\n",
    "PAD_TOKEN = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b75aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus_books (/home/users/ujjwal.upadhyay/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87a26c17c034896b00a009e7513a4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '23203',\n",
       " 'translation': {'en': 'Ouvrant la fenetre aussi doucement que possible, ils attendirent le passage d’un nuage épais qui vint obscurcir la nuit, puis l’un derriere l’autre ils se mirent a traverser le jardin ; rampant, trébuchant, retenant avec soin leur respiration, ils parvinrent a gagner l’abri de la haie, qu’ils voulaient longer jusqu’a la breche donnant dans les champs.',\n",
       "  'fr': 'Opening the window very slowly and carefully, they waited until a dark cloud had somewhat obscured the night, and then one by one passed through into the little garden. With bated breath and crouching figures they stumbled across it, and gained the shelter of the hedge, which they skirted until they came to the gap which opened into the cornfields.'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "books.cleanup_cache_files()\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)\n",
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9781853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1553078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b0188d10874f5587f67517d877a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f456b8f1bc4af38f73837202fcfec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True, padding=True, return_special_tokens_mask=True, return_tensors='pt')\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0573f5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'translation', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_books['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1728fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_books['train'][0:10]['input_ids']), len(tokenized_books['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3954b862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hello',\n",
       " ',',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'all',\n",
       " '!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Hello, y'all! How are you?\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a7fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_batches(num_batch, batch_size):\n",
    "    src_tokens, trg_tokens, src_masks, trg_masks, trg_gts = [], [], [], [], []\n",
    "    for i in tqdm(range(num_batch)):\n",
    "        src_tkn = torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['input_ids']).to(torch.IntTensor())\n",
    "        trg_tkn = torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['labels']).to(torch.IntTensor())\n",
    "#         src_mask = torch.BoolTensor(tokenized_books['train'][0:BATCH_SIZE]['attention_mask']).view(BATCH_SIZE, 1, 1, -1)\n",
    "#         trg_mask = torch.BoolTensor(torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['labels'])==1).view(BATCH_SIZE, 1, 1, -1)\n",
    "        src_mask = torch.BoolTensor(tokenized_books['train'][0:BATCH_SIZE]['attention_mask']).view(BATCH_SIZE, -1)\n",
    "        trg_mask = torch.BoolTensor(torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['labels'])==1).view(BATCH_SIZE, -1)\n",
    "        \n",
    "        label_smoothing = LabelSmoothingDistribution(BASELINE_MODEL_LABEL_SMOOTHING_VALUE, tokenizer.pad_token_id, tokenizer.vocab_size, \"cpu\")\n",
    "        trg_gt = label_smoothing(trg_tkn[:, :].reshape(-1, 1))\n",
    "        \n",
    "        src_tokens.append(src_tkn)\n",
    "        trg_tokens.append(trg_tkn)\n",
    "        src_masks.append(src_mask)\n",
    "        trg_masks.append(trg_mask)\n",
    "        trg_gts.append(trg_gt)\n",
    "\n",
    "    return src_tokens, trg_tokens, src_masks, trg_masks, trg_gts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be492d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25d30dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters: 84767546\n"
     ]
    }
   ],
   "source": [
    "from transformer import Transformer\n",
    "trans = Transformer(model_dimension=512, src_vocab_size=tokenizer.vocab_size, \n",
    "                    trg_vocab_size=tokenizer.vocab_size, \n",
    "                    number_of_heads=8, number_of_layers=4, mem_size=16,\n",
    "                    dropout_probability=0.1, log_attention_weights=False)\n",
    "trans.train()\n",
    "print(\"No of parameters:\", sum(dict((p.data_ptr(), p.numel()) for p in trans.parameters()).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90ed7c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0fb337be44073922b401099291b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC TKN: torch.Size([10, 128])\n",
      "TRG TKN: torch.Size([10, 128])\n",
      "SRC MASK: torch.Size([10, 128])\n",
      "TRG_MASK: torch.Size([10, 128])\n",
      "TRG GT: torch.Size([1280, 30522])\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "src_tokens, trg_tokens, src_masks, trg_masks, trg_gts = create_n_batches(num_batch=50, batch_size=BATCH_SIZE)\n",
    "\n",
    "# src_tkn = torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['input_ids']).to(torch.IntTensor())\n",
    "# trg_tkn = torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['labels']).to(torch.IntTensor())\n",
    "# src_mask = torch.BoolTensor(tokenized_books['train'][0:BATCH_SIZE]['attention_mask']).view(BATCH_SIZE, 1, 1, -1)\n",
    "# trg_mask = torch.BoolTensor(torch.Tensor(tokenized_books['train'][0:BATCH_SIZE]['labels'])==1).view(BATCH_SIZE, 1, 1, -1)\n",
    "\n",
    "# label_smoothing = LabelSmoothingDistribution(BASELINE_MODEL_LABEL_SMOOTHING_VALUE, tokenizer.pad_token_id, tokenizer.vocab_size, \"cpu\")\n",
    "# trg_gt = label_smoothing(trg_tkn[:, :].reshape(-1, 1))\n",
    "\n",
    "optimizer = optim.AdamW(trans.parameters(), lr=5e-4)\n",
    "kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "init_src_memory = trans.src_memory.clone()\n",
    "init_trg_memory = trans.trg_memory.clone()\n",
    "\n",
    "print(f\"SRC TKN: {src_tokens[0].shape}\\nTRG TKN: {trg_tokens[0].shape}\\nSRC MASK: {src_masks[0].shape}\\nTRG_MASK: {trg_masks[0].shape}\\nTRG GT: {trg_gts[0].shape}\")\n",
    "src_tkn, trg_tkn, src_mask, trg_mask, trg_gt = src_tokens[0], trg_tokens[0], src_masks[0], trg_masks[0], trg_gts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438f9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_state, log_probab = trans(src_tkn, trg_tkn, src_mask, trg_mask)\n",
    "# h_state.shape, log_probab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "304632d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9ac91fad3f412982b3fce5eb0f6e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 2-D and 2-D tensors respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     h_state, log_probab \u001b[38;5;241m=\u001b[39m \u001b[43mtrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m kl_div_loss(log_probab, trg_gts[i])\n\u001b[1;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/trans/transformer.py:78\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_token_ids_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrg_log_probs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_hidden_state, trg_token_ids_batch, trg_mask)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrg_log_probs\n",
      "File \u001b[0;32m~/lab/trans/transformer.py:86\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[0;34m(self, src_token_ids_batch, src_mask)\u001b[0m\n\u001b[1;32m     84\u001b[0m src_embeddings_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embedding(src_token_ids_batch)  \u001b[38;5;66;03m# get embedding vectors for src token ids\u001b[39;00m\n\u001b[1;32m     85\u001b[0m src_embeddings_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pos_embedding(src_embeddings_batch)  \u001b[38;5;66;03m# add positional embedding\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embeddings_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward pass through the encoder\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/trans/transformer.py:129\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src_embeddings_batch, src_mask)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    128\u001b[0m         src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_memory \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_representations_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Not mentioned explicitly in the paper (a consequence of using LayerNorm before instead of after the sublayer\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# check out the SublayerLogic module)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_memory\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/trans/transformer.py:159\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, src_memory, src_representations_batch, src_mask)\u001b[0m\n\u001b[1;32m    157\u001b[0m src_representations_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(src_representations_batch, encoder_self_attention)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_memory_update:\n\u001b[0;32m--> 159\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_representations_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     src_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_attention(src_memory, src_representations_batch, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    161\u001b[0m     src_representations_batch \u001b[38;5;241m=\u001b[39m hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/trans/transformer.py:218\u001b[0m, in \u001b[0;36mOutputAttention.forward\u001b[0;34m(self, memory, representations_batch, mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, memory, representations_batch, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#         memory = memory.repeat(representations_batch.shape[0],1,1)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;66;03m# Define anonymous (lambda) function which only takes src_representations_batch (srb) as input,\u001b[39;00m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# this way we have a uniform interface for the sublayer logic.\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         hidden_state, self_attn_wts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_headed_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepresentations_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m         hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_norm(hidden_state, representations_batch)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m#         memory = memory.mean(axis=0).unsqueeze(dim=0)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/functional.py:5005\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   4975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tens_ops):\n\u001b[1;32m   4976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4977\u001b[0m         multi_head_attention_forward,\n\u001b[1;32m   4978\u001b[0m         tens_ops,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5002\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   5003\u001b[0m     )\n\u001b[0;32m-> 5005\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[43m_mha_shape_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5007\u001b[0m \u001b[38;5;66;03m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001b[39;00m\n\u001b[1;32m   5008\u001b[0m \u001b[38;5;66;03m# is batched, run the computation and before returning squeeze the\u001b[39;00m\n\u001b[1;32m   5009\u001b[0m \u001b[38;5;66;03m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001b[39;00m\n\u001b[1;32m   5010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5011\u001b[0m     \u001b[38;5;66;03m# unsqueeze if the input is unbatched\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/torch/nn/functional.py:4849\u001b[0m, in \u001b[0;36m_mha_shape_check\u001b[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001b[0m\n\u001b[1;32m   4846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   4847\u001b[0m     \u001b[38;5;66;03m# Batched Inputs\u001b[39;00m\n\u001b[1;32m   4848\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 4849\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \\\n\u001b[1;32m   4850\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched (3-D) `query`, expected `key` and `value` to be 3-D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4851\u001b[0m          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensors respectively\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4853\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m key_padding_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \\\n\u001b[1;32m   4854\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4855\u001b[0m              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_padding_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 2-D and 2-D tensors respectively"
     ]
    }
   ],
   "source": [
    "gradient_accumulation_steps = 5\n",
    "for i in tqdm(range(50)):\n",
    "    h_state, log_probab = trans(src_tokens[i], trg_tokens[i], src_masks[i], trg_masks[i])\n",
    "    loss = kl_div_loss(log_probab, trg_gts[i])\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward(retain_graph=True)\n",
    "    if (i + 1) % gradient_accumulation_steps == 0:\n",
    "        print(f\"EPOCH:{i+1} - LOSS:{loss}\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d218c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check memory changes\n",
      "\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"check memory changes\\n\")\n",
    "print((trans.src_memory == init_src_memory).sum())\n",
    "print((trans.trg_memory == init_trg_memory).sum())\n",
    "torch.isnan(trans.trg_memory).sum(), torch.isnan(trans.src_memory).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecc27246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128]) torch.Size([3, 128]) torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "bsz_test = 3\n",
    "idx = 10\n",
    "src_tkn_test = torch.Tensor(tokenized_books['train'][idx:idx+bsz_test]['input_ids']).to(torch.IntTensor())\n",
    "trg_tkn_test = torch.Tensor(tokenized_books['train'][idx:idx+bsz_test]['labels']).to(torch.IntTensor())\n",
    "src_mask_test = torch.BoolTensor(tokenized_books['train'][idx:idx+bsz_test]['attention_mask']).view(bsz_test, -1) \n",
    "print(src_tkn_test.shape, trg_tkn_test.shape, src_mask_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb2b026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 512]), torch.Size([3, 512]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.eval()\n",
    "with torch.no_grad():\n",
    "    h_state_test = trans.encode(src_tkn_test, src_mask_test)\n",
    "\n",
    "h_state_test.shape, h_state_test.argmax(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ce6e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = h_state_test.argmax(axis=1).numpy()\n",
    "# type(out[0]), len(out[0]), out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e5d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d511394c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map['cls_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de49233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 101, 7592, 102]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(f\"{tokenizer.special_tokens_map['cls_token']}Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa2ec0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 102)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "a0813567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grey'"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(7592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4be4907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101],\n",
       "        [101],\n",
       "        [101],\n",
       "        [101]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "69481fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_size = 4\n",
    "src_representations_batch = h_state_test\n",
    "src_representations_batch = src_representations_batch.repeat(1, beam_size, 1).view(beam_size*batch_size, -1, model_dimension)\n",
    "src_representations_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d1507d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 0])"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_token_ids_batch = trg_token_ids_batch.repeat(beam_size, 1)\n",
    "trg_token_ids_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "c461a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_log_probs = torch.zeros((batch_size * beam_size, 1), device=device)\n",
    "had_eos = [[False] for _ in range(hypotheses_log_probs.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11fd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3ad73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f088152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b00e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f87f2622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100, 100])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, 100, 100), device=device) == 1).transpose(2, 3)\n",
    "trg_no_look_forward_mask.shape\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cd7a2815",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [149], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     num_trg_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(trg_padding_mask\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trg_mask, num_trg_tokens\n\u001b[0;32m---> 18\u001b[0m trg_mask, num_trg_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mget_masks_and_count_tokens_trg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_token_ids_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m trg_mask\u001b[38;5;241m.\u001b[39mshape, trg_mask, num_trg_tokens\n",
      "Cell \u001b[0;32mIn [149], line 12\u001b[0m, in \u001b[0;36mget_masks_and_count_tokens_trg\u001b[0;34m(trg_token_ids_batch, pad_token_id)\u001b[0m\n\u001b[1;32m      9\u001b[0m     trg_no_look_forward_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, sequence_length, sequence_length), device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     trg_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtrg_padding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrg_no_look_forward_mask\u001b[49m  \u001b[38;5;66;03m# final shape = (B, 1, T, T)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     trg_mask = trg_padding_mask\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     num_trg_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(trg_padding_mask\u001b[38;5;241m.\u001b[39mlong())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "def get_masks_and_count_tokens_trg(trg_token_ids_batch, pad_token_id):\n",
    "    batch_size = trg_token_ids_batch.shape[0]\n",
    "    device = trg_token_ids_batch.device\n",
    "\n",
    "    # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "    # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "    sequence_length = trg_token_ids_batch.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "    trg_padding_mask = (trg_token_ids_batch != pad_token_id).view(batch_size, -1)  # shape = (B, T)\n",
    "    trg_no_look_forward_mask = torch.triu(torch.ones((1,1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "#     # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "    trg_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "#     trg_mask = trg_padding_mask\n",
    "    num_trg_tokens = torch.sum(trg_padding_mask.long())\n",
    "\n",
    "    return trg_mask, num_trg_tokens\n",
    "\n",
    "trg_mask, num_trg_tokens = get_masks_and_count_tokens_trg(trg_token_ids_batch, tokenizer.pad_token_id)\n",
    "trg_mask.shape, trg_mask, num_trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ff7f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[101, 102]]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(trans.parameters()).device\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "batch_size, S, model_dimension = h_state_test.shape\n",
    "target_multiple_hypotheses_tokens = [[\"[CLS]\"] for _ in range(1)]\n",
    "trg_token_ids_batch = torch.tensor([[101, 102] for tokens in target_multiple_hypotheses_tokens], device=device)\n",
    "trg_token_ids_batch, trg_token_ids_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8113976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "h_state_test = h_state_test.repeat(1,beam_size,1).view(beam_size*bsz_test, -1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3e83d7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128, 512])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_state_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b69a573d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_token_ids_batch = trg_token_ids_batch.repeat(beam_size, 1)\n",
    "trg_token_ids_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b6c1f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_log_probs = torch.zeros((bsz_test * beam_size, 1), device=device)\n",
    "had_eos = [[False] for _ in range(hypotheses_log_probs.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb88d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.decode(h_state_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "5e4aabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decoding(beam_size, tokenizer, transformer, src_representations_batch, src_mask, max_target_tokens=100):\n",
    "    device = next(transformer.parameters()).device\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Initial prompt is the beginning/start of the sentence token. Make it compatible shape with source batch => (B,1)\n",
    "    batch_size, S, model_dimension = src_representations_batch.shape\n",
    "    target_multiple_hypotheses_tokens = [[\"[CLS]\"] for _ in range(batch_size)]\n",
    "    trg_token_ids_batch = torch.tensor([[101] for tokens in target_multiple_hypotheses_tokens], device=device)\n",
    "    \n",
    "    # Repeat so that source sentence representations are repeated contiguously, say we have [s1, s2] we want\n",
    "    # [s1, s1, s2, s2] and not [s1, s2, s1, s2] where s1 is single sentence representation with shape=(S, D)\n",
    "    # where S - max source token-sequence length, D - model dimension\n",
    "    src_representations_batch = src_representations_batch.repeat(1, beam_size, 1).view(beam_size*batch_size, -1, model_dimension)\n",
    "    trg_token_ids_batch = trg_token_ids_batch.repeat(beam_size, 1)\n",
    "\n",
    "    hypotheses_log_probs = torch.zeros((batch_size * beam_size, 1), device=device)\n",
    "    had_eos = [[False] for _ in range(hypotheses_log_probs.shape[0])]\n",
    "\n",
    "    while True:\n",
    "        trg_mask, _ = get_masks_and_count_tokens_trg(trg_token_ids_batch, pad_token_id)\n",
    "        # Shape = (B*BS*T, V) T - current token-sequence length, V - target vocab size, BS - beam size, B - batch\n",
    "        predicted_log_distributions = transformer.decode(src_representations_batch, trg_token_ids_batch, trg_mask)\n",
    "\n",
    "        # Extract only the indices of last token for every target sentence (we take every T-th token)\n",
    "        # Shape = (B*BS, V)\n",
    "        num_of_trg_tokens = trg_token_ids_batch.shape[-1]\n",
    "        predicted_log_distributions = predicted_log_distributions[num_of_trg_tokens - 1::num_of_trg_tokens]\n",
    "\n",
    "        # This time extract beam_size number of highest probability tokens (compare to greedy's arg max)\n",
    "        # Shape = (B*BS, BS)\n",
    "        latest_token_log_probs, most_probable_token_indices = torch.topk(predicted_log_distributions, beam_size, dim=-1, sorted=True)\n",
    "\n",
    "        # Don't update the hypothesis which had EOS already (pruning)\n",
    "        latest_token_log_probs.masked_fill(torch.tensor(had_eos == True), float(\"-inf\"))\n",
    "\n",
    "        # Calculate probabilities for every beam hypothesis (since we have log prob we add instead of multiply)\n",
    "        # Shape = (B*BS, BS)\n",
    "        hypotheses_pool_log_probs = hypotheses_log_probs + latest_token_log_probs\n",
    "        # Shape = (B, BS, BS)\n",
    "        most_probable_token_indices = most_probable_token_indices.view(batch_size, beam_size, beam_size)\n",
    "        hypotheses_pool_log_probs = hypotheses_pool_log_probs.view(batch_size, beam_size, beam_size)\n",
    "        # Shape = (B, BS*BS)\n",
    "        hypotheses_pool_log_probs = torch.flatten(hypotheses_pool_log_probs, start_dim=-1)\n",
    "\n",
    "        # Figure out indices of beam_size most probably hypothesis for every target sentence in the batch\n",
    "        # Shape = (B, BS)\n",
    "        new_hypothesis_log_probs, next_hypothesis_indices = torch.topk(hypotheses_pool_log_probs, beam_size, dim=-1, sorted=True)\n",
    "\n",
    "        # Create new target ids batch\n",
    "        hypotheses_log_probs_tmp = torch.empty((batch_size * beam_size, 1))\n",
    "\n",
    "        T = trg_token_ids_batch.shape[-1]\n",
    "        new_trg_token_ids_batch = torch.empty((batch_size * beam_size, T + 1))\n",
    "\n",
    "        next_hypothesis_indices = next_hypothesis_indices.cpu().numpy()\n",
    "        # Prepare new hypotheses for the next iteration\n",
    "        for b_idx, indices in enumerate(next_hypothesis_indices):\n",
    "            for h_idx, token_index in indices:\n",
    "                row, column = token_index / beam_size, token_index % beam_size\n",
    "                hypothesis_index = b_idx * beam_size + h_idx\n",
    "\n",
    "                new_token_id = most_probable_token_indices[b_idx, row, column]\n",
    "                if had_eos[hypothesis_index]:\n",
    "                    new_trg_token_ids_batch[hypothesis_index, :-1] = trg_token_ids_batch[hypothesis_index, :]\n",
    "                else:\n",
    "                    new_trg_token_ids_batch[hypothesis_index, :-1] = trg_token_ids_batch[b_idx * beam_size + row, :]\n",
    "                    new_trg_token_ids_batch[hypothesis_index, -1] = new_token_id\n",
    "\n",
    "                if had_eos[hypothesis_index]:\n",
    "                    hypotheses_log_probs_tmp[hypothesis_index] = hypotheses_log_probs[hypothesis_index]\n",
    "                else:\n",
    "                    hypotheses_log_probs_tmp[hypothesis_index] = new_hypothesis_log_probs[hypothesis_index]\n",
    "\n",
    "                if new_token_id == tokenizer.eos_token_id:\n",
    "                    had_eos[hypothesis_index] = True\n",
    "\n",
    "        # Update the current hypothesis probabilities\n",
    "        hypotheses_log_probs = hypotheses_log_probs_tmp\n",
    "        trg_token_ids_batch = new_trg_token_ids_batch\n",
    "\n",
    "        if all(had_eos) or num_of_trg_tokens == max_target_tokens:\n",
    "            break\n",
    "\n",
    "    #\n",
    "    # Selection and post-processing\n",
    "    #\n",
    "\n",
    "    target_multiple_hypotheses_tokens = []\n",
    "    trg_token_ids_batch_numpy = trg_token_ids_batch.cpu().numpy()\n",
    "    for hypothesis_ids in trg_token_ids_batch_numpy:\n",
    "        target_multiple_hypotheses_tokens.append([tokenizer.decode(token_id) for token_id in hypothesis_ids])\n",
    "\n",
    "    # Step 1: Select the most probable hypothesis out of beam_size hypotheses for each target sentence\n",
    "    hypotheses_log_probs = hypotheses_log_probs.view(batch_size, beam_size)\n",
    "    most_probable_hypotheses_indices = torch.argmax(hypotheses_log_probs, dim=-1).cpu().numpy()\n",
    "    target_sentences_tokens = []\n",
    "    for b_idx, index in enumerate(most_probable_hypotheses_indices):\n",
    "        target_sentences_tokens.append(target_multiple_hypotheses_tokens[b_idx * beam_size + index])\n",
    "\n",
    "    # Step 2: Post process the sentences - remove everything after the EOS token\n",
    "    target_sentences_tokens_post = []\n",
    "    for target_sentence_tokens in target_sentences_tokens:\n",
    "        try:\n",
    "            target_index = target_sentence_tokens.index(EOS_TOKEN) + 1\n",
    "        except:\n",
    "            target_index = None\n",
    "\n",
    "        target_sentence_tokens = target_sentence_tokens[:target_index]\n",
    "        target_sentences_tokens_post.append(target_sentence_tokens)\n",
    "\n",
    "    return target_sentences_tokens_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b4bd4791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
